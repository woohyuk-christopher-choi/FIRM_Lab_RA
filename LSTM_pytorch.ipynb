{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.layers import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>trades</th>\n",
       "      <th>VIX</th>\n",
       "      <th>Bitcoin_TransactionFee</th>\n",
       "      <th>volatility</th>\n",
       "      <th>GoogleTrend</th>\n",
       "      <th>DowJones</th>\n",
       "      <th>EuroStoxx50</th>\n",
       "      <th>Nasdaq</th>\n",
       "      <th>OilWTI</th>\n",
       "      <th>SPX</th>\n",
       "      <th>TradePermin</th>\n",
       "      <th>Dominance</th>\n",
       "      <th>Gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0.926926</td>\n",
       "      <td>0.442124</td>\n",
       "      <td>0.908569</td>\n",
       "      <td>0.569227</td>\n",
       "      <td>0.042299</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.547234</td>\n",
       "      <td>0.571035</td>\n",
       "      <td>0.582848</td>\n",
       "      <td>0.533921</td>\n",
       "      <td>0.576066</td>\n",
       "      <td>0.535315</td>\n",
       "      <td>0.130527</td>\n",
       "      <td>0.535216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.486714</td>\n",
       "      <td>0.425847</td>\n",
       "      <td>0.537085</td>\n",
       "      <td>0.058249</td>\n",
       "      <td>0.020318</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.574495</td>\n",
       "      <td>0.573304</td>\n",
       "      <td>0.714814</td>\n",
       "      <td>0.514008</td>\n",
       "      <td>0.514174</td>\n",
       "      <td>0.547026</td>\n",
       "      <td>0.590266</td>\n",
       "      <td>0.130527</td>\n",
       "      <td>0.489068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.536732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.023233</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551046</td>\n",
       "      <td>0.605995</td>\n",
       "      <td>0.638188</td>\n",
       "      <td>0.582004</td>\n",
       "      <td>0.493552</td>\n",
       "      <td>0.583846</td>\n",
       "      <td>0.621124</td>\n",
       "      <td>0.130527</td>\n",
       "      <td>0.532305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.486078</td>\n",
       "      <td>0.903958</td>\n",
       "      <td>0.524104</td>\n",
       "      <td>0.050138</td>\n",
       "      <td>0.018765</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907375</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>0.492669</td>\n",
       "      <td>0.529880</td>\n",
       "      <td>0.526148</td>\n",
       "      <td>0.512233</td>\n",
       "      <td>0.518033</td>\n",
       "      <td>0.419330</td>\n",
       "      <td>0.130527</td>\n",
       "      <td>0.532305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>0.523761</td>\n",
       "      <td>0.483127</td>\n",
       "      <td>0.389201</td>\n",
       "      <td>0.257298</td>\n",
       "      <td>0.032264</td>\n",
       "      <td>0.013878</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.911113</td>\n",
       "      <td>0.480700</td>\n",
       "      <td>0.492669</td>\n",
       "      <td>0.529880</td>\n",
       "      <td>0.526148</td>\n",
       "      <td>0.512233</td>\n",
       "      <td>0.518033</td>\n",
       "      <td>0.426805</td>\n",
       "      <td>0.072534</td>\n",
       "      <td>0.532305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Open_time      Open      High       Low     Close    Volume    trades  \\\n",
       "0  2018-01-03  0.926926  0.442124  0.908569  0.569227  0.042299  0.019350   \n",
       "1  2018-01-04  0.564277  0.486714  0.425847  0.537085  0.058249  0.020318   \n",
       "2  2018-01-05  0.536732  1.000000  0.702397  1.000000  0.063900  0.023233   \n",
       "3  2018-01-06  1.000000  0.486078  0.903958  0.524104  0.050138  0.018765   \n",
       "4  2018-01-07  0.523761  0.483127  0.389201  0.257298  0.032264  0.013878   \n",
       "\n",
       "        VIX  Bitcoin_TransactionFee  volatility  GoogleTrend  DowJones  \\\n",
       "0  0.000000                     1.0    1.000000     0.586219  0.547234   \n",
       "1  0.001940                     1.0    1.000000     0.574495  0.573304   \n",
       "2  0.001940                     1.0    1.000000     0.551046  0.605995   \n",
       "3  0.010254                     1.0    0.907375     0.515873  0.492669   \n",
       "4  0.010254                     1.0    0.911113     0.480700  0.492669   \n",
       "\n",
       "   EuroStoxx50    Nasdaq    OilWTI       SPX  TradePermin  Dominance      Gold  \n",
       "0     0.571035  0.582848  0.533921  0.576066     0.535315   0.130527  0.535216  \n",
       "1     0.714814  0.514008  0.514174  0.547026     0.590266   0.130527  0.489068  \n",
       "2     0.638188  0.582004  0.493552  0.583846     0.621124   0.130527  0.532305  \n",
       "3     0.529880  0.526148  0.512233  0.518033     0.419330   0.130527  0.532305  \n",
       "4     0.529880  0.526148  0.512233  0.518033     0.426805   0.072534  0.532305  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./Data/MinMaxScaled_Winsorized_Feature_20231027.csv')\n",
    "# Drop the specified columns from the dataset\n",
    "data = data.drop(columns=[\n",
    "    'quote_av',\n",
    "    'tb_base_av',\n",
    "    'tb_quote_av',\n",
    "])\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2067 entries, 0 to 2066\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Open_time               2067 non-null   object \n",
      " 1   Open                    2067 non-null   float64\n",
      " 2   High                    2067 non-null   float64\n",
      " 3   Low                     2067 non-null   float64\n",
      " 4   Close                   2067 non-null   float64\n",
      " 5   Volume                  2067 non-null   float64\n",
      " 6   trades                  2067 non-null   float64\n",
      " 7   VIX                     2067 non-null   float64\n",
      " 8   Bitcoin_TransactionFee  2067 non-null   float64\n",
      " 9   volatility              2067 non-null   float64\n",
      " 10  GoogleTrend             2067 non-null   float64\n",
      " 11  DowJones                2067 non-null   float64\n",
      " 12  EuroStoxx50             2067 non-null   float64\n",
      " 13  Nasdaq                  2067 non-null   float64\n",
      " 14  OilWTI                  2067 non-null   float64\n",
      " 15  SPX                     2067 non-null   float64\n",
      " 16  TradePermin             2067 non-null   float64\n",
      " 17  Dominance               2067 non-null   float64\n",
      " 18  Gold                    2067 non-null   float64\n",
      "dtypes: float64(18), object(1)\n",
      "memory usage: 306.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_close = data['Close'].values\n",
    "dates = pd.to_datetime(data['Open_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>trades</th>\n",
       "      <th>VIX</th>\n",
       "      <th>Bitcoin_TransactionFee</th>\n",
       "      <th>volatility</th>\n",
       "      <th>GoogleTrend</th>\n",
       "      <th>DowJones</th>\n",
       "      <th>EuroStoxx50</th>\n",
       "      <th>Nasdaq</th>\n",
       "      <th>OilWTI</th>\n",
       "      <th>SPX</th>\n",
       "      <th>TradePermin</th>\n",
       "      <th>Dominance</th>\n",
       "      <th>Gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.926926</td>\n",
       "      <td>0.442124</td>\n",
       "      <td>0.908569</td>\n",
       "      <td>0.569227</td>\n",
       "      <td>0.042299</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.547234</td>\n",
       "      <td>0.571035</td>\n",
       "      <td>0.582848</td>\n",
       "      <td>0.533921</td>\n",
       "      <td>0.576066</td>\n",
       "      <td>0.535315</td>\n",
       "      <td>0.130527</td>\n",
       "      <td>0.535216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.564277</td>\n",
       "      <td>0.486714</td>\n",
       "      <td>0.425847</td>\n",
       "      <td>0.537085</td>\n",
       "      <td>0.058249</td>\n",
       "      <td>0.020318</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.574495</td>\n",
       "      <td>0.573304</td>\n",
       "      <td>0.714814</td>\n",
       "      <td>0.514008</td>\n",
       "      <td>0.514174</td>\n",
       "      <td>0.547026</td>\n",
       "      <td>0.590266</td>\n",
       "      <td>0.130527</td>\n",
       "      <td>0.489068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.536732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.023233</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551046</td>\n",
       "      <td>0.605995</td>\n",
       "      <td>0.638188</td>\n",
       "      <td>0.582004</td>\n",
       "      <td>0.493552</td>\n",
       "      <td>0.583846</td>\n",
       "      <td>0.621124</td>\n",
       "      <td>0.130527</td>\n",
       "      <td>0.532305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.486078</td>\n",
       "      <td>0.903958</td>\n",
       "      <td>0.524104</td>\n",
       "      <td>0.050138</td>\n",
       "      <td>0.018765</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907375</td>\n",
       "      <td>0.515873</td>\n",
       "      <td>0.492669</td>\n",
       "      <td>0.529880</td>\n",
       "      <td>0.526148</td>\n",
       "      <td>0.512233</td>\n",
       "      <td>0.518033</td>\n",
       "      <td>0.419330</td>\n",
       "      <td>0.130527</td>\n",
       "      <td>0.532305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.523761</td>\n",
       "      <td>0.483127</td>\n",
       "      <td>0.389201</td>\n",
       "      <td>0.257298</td>\n",
       "      <td>0.032264</td>\n",
       "      <td>0.013878</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911113</td>\n",
       "      <td>0.480700</td>\n",
       "      <td>0.492669</td>\n",
       "      <td>0.529880</td>\n",
       "      <td>0.526148</td>\n",
       "      <td>0.512233</td>\n",
       "      <td>0.518033</td>\n",
       "      <td>0.426805</td>\n",
       "      <td>0.072534</td>\n",
       "      <td>0.532305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>0.488047</td>\n",
       "      <td>0.506510</td>\n",
       "      <td>0.491951</td>\n",
       "      <td>0.509925</td>\n",
       "      <td>0.031107</td>\n",
       "      <td>0.043323</td>\n",
       "      <td>0.164349</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.100026</td>\n",
       "      <td>0.468975</td>\n",
       "      <td>0.574360</td>\n",
       "      <td>0.672967</td>\n",
       "      <td>0.583431</td>\n",
       "      <td>0.510437</td>\n",
       "      <td>0.574422</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.398208</td>\n",
       "      <td>0.479670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>0.509599</td>\n",
       "      <td>0.509407</td>\n",
       "      <td>0.478626</td>\n",
       "      <td>0.498667</td>\n",
       "      <td>0.062257</td>\n",
       "      <td>0.065709</td>\n",
       "      <td>0.164349</td>\n",
       "      <td>0.042706</td>\n",
       "      <td>0.099933</td>\n",
       "      <td>0.527597</td>\n",
       "      <td>0.574360</td>\n",
       "      <td>0.672967</td>\n",
       "      <td>0.583431</td>\n",
       "      <td>0.510437</td>\n",
       "      <td>0.574422</td>\n",
       "      <td>0.020247</td>\n",
       "      <td>0.398208</td>\n",
       "      <td>0.479670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>0.498354</td>\n",
       "      <td>0.855231</td>\n",
       "      <td>0.504577</td>\n",
       "      <td>0.765847</td>\n",
       "      <td>0.213876</td>\n",
       "      <td>0.144722</td>\n",
       "      <td>0.146888</td>\n",
       "      <td>0.058046</td>\n",
       "      <td>0.156660</td>\n",
       "      <td>0.691739</td>\n",
       "      <td>0.601978</td>\n",
       "      <td>0.596692</td>\n",
       "      <td>0.676966</td>\n",
       "      <td>0.523665</td>\n",
       "      <td>0.675428</td>\n",
       "      <td>0.077595</td>\n",
       "      <td>0.398208</td>\n",
       "      <td>0.635801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>0.765207</td>\n",
       "      <td>0.429252</td>\n",
       "      <td>0.673863</td>\n",
       "      <td>0.429149</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.087229</td>\n",
       "      <td>0.131091</td>\n",
       "      <td>0.050959</td>\n",
       "      <td>0.158820</td>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.512095</td>\n",
       "      <td>0.465197</td>\n",
       "      <td>0.552157</td>\n",
       "      <td>0.513671</td>\n",
       "      <td>0.544631</td>\n",
       "      <td>0.033075</td>\n",
       "      <td>0.398208</td>\n",
       "      <td>0.664190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>0.428922</td>\n",
       "      <td>0.463128</td>\n",
       "      <td>0.281938</td>\n",
       "      <td>0.275405</td>\n",
       "      <td>0.145596</td>\n",
       "      <td>0.119964</td>\n",
       "      <td>0.122499</td>\n",
       "      <td>0.077834</td>\n",
       "      <td>0.180330</td>\n",
       "      <td>0.562771</td>\n",
       "      <td>0.439700</td>\n",
       "      <td>0.444108</td>\n",
       "      <td>0.507398</td>\n",
       "      <td>0.538795</td>\n",
       "      <td>0.478099</td>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.398208</td>\n",
       "      <td>0.444503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2067 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open      High       Low     Close    Volume    trades       VIX  \\\n",
       "0     0.926926  0.442124  0.908569  0.569227  0.042299  0.019350  0.000000   \n",
       "1     0.564277  0.486714  0.425847  0.537085  0.058249  0.020318  0.001940   \n",
       "2     0.536732  1.000000  0.702397  1.000000  0.063900  0.023233  0.001940   \n",
       "3     1.000000  0.486078  0.903958  0.524104  0.050138  0.018765  0.010254   \n",
       "4     0.523761  0.483127  0.389201  0.257298  0.032264  0.013878  0.010254   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2062  0.488047  0.506510  0.491951  0.509925  0.031107  0.043323  0.164349   \n",
       "2063  0.509599  0.509407  0.478626  0.498667  0.062257  0.065709  0.164349   \n",
       "2064  0.498354  0.855231  0.504577  0.765847  0.213876  0.144722  0.146888   \n",
       "2065  0.765207  0.429252  0.673863  0.429149  0.099767  0.087229  0.131091   \n",
       "2066  0.428922  0.463128  0.281938  0.275405  0.145596  0.119964  0.122499   \n",
       "\n",
       "      Bitcoin_TransactionFee  volatility  GoogleTrend  DowJones  EuroStoxx50  \\\n",
       "0                   1.000000    1.000000     0.586219  0.547234     0.571035   \n",
       "1                   1.000000    1.000000     0.574495  0.573304     0.714814   \n",
       "2                   1.000000    1.000000     0.551046  0.605995     0.638188   \n",
       "3                   1.000000    0.907375     0.515873  0.492669     0.529880   \n",
       "4                   1.000000    0.911113     0.480700  0.492669     0.529880   \n",
       "...                      ...         ...          ...       ...          ...   \n",
       "2062                0.029851    0.100026     0.468975  0.574360     0.672967   \n",
       "2063                0.042706    0.099933     0.527597  0.574360     0.672967   \n",
       "2064                0.058046    0.156660     0.691739  0.601978     0.596692   \n",
       "2065                0.050959    0.158820     0.586219  0.512095     0.465197   \n",
       "2066                0.077834    0.180330     0.562771  0.439700     0.444108   \n",
       "\n",
       "        Nasdaq    OilWTI       SPX  TradePermin  Dominance      Gold  \n",
       "0     0.582848  0.533921  0.576066     0.535315   0.130527  0.535216  \n",
       "1     0.514008  0.514174  0.547026     0.590266   0.130527  0.489068  \n",
       "2     0.582004  0.493552  0.583846     0.621124   0.130527  0.532305  \n",
       "3     0.526148  0.512233  0.518033     0.419330   0.130527  0.532305  \n",
       "4     0.526148  0.512233  0.518033     0.426805   0.072534  0.532305  \n",
       "...        ...       ...       ...          ...        ...       ...  \n",
       "2062  0.583431  0.510437  0.574422     0.000702   0.398208  0.479670  \n",
       "2063  0.583431  0.510437  0.574422     0.020247   0.398208  0.479670  \n",
       "2064  0.676966  0.523665  0.675428     0.077595   0.398208  0.635801  \n",
       "2065  0.552157  0.513671  0.544631     0.033075   0.398208  0.664190  \n",
       "2066  0.507398  0.538795  0.478099     0.065211   0.398208  0.444503  \n",
       "\n",
       "[2067 rows x 18 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables for training\n",
    "cols = list(data)[1:19]\n",
    "data_for_train = data[cols].astype(float)\n",
    "\n",
    "data_for_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1963, 18), (104, 18))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split to train data and test data\n",
    "n_train = int(0.95*data_for_train.shape[0])\n",
    "train_data = data_for_train[0: n_train]\n",
    "train_dates = dates[0: n_train]\n",
    "\n",
    "test_data = data_for_train[n_train:]\n",
    "test_dates = dates[n_train:]\n",
    "\n",
    "train_data.shape, test_data.shape # 2D Arrays , (1860,18) , (207,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class windowDataset(Dataset):\n",
    "    def __init__(self, data, target, input_window, output_window, stride=1):\n",
    "        data = data.values \n",
    "        L, features = data.shape\n",
    "        num_samples = (L - input_window - output_window) // stride + 1\n",
    "\n",
    "        X = np.zeros([input_window, num_samples, features])\n",
    "        Y = np.zeros([output_window, num_samples, 1])  # 'Close' 열만을 대상으로 하므로 1로 설정\n",
    "\n",
    "        for i in np.arange(num_samples):\n",
    "            start_x = stride*i\n",
    "            end_x = start_x + input_window\n",
    "            X[:,i,:] = data[start_x:end_x, :]\n",
    "\n",
    "            start_y = stride*i + input_window\n",
    "            end_y = start_y + output_window\n",
    "            Y[:,i,0] = target[start_y:end_y]  # 'Close' 열만을 대상으로 합니다.\n",
    "\n",
    "        self.x = X.transpose((1,0,2))\n",
    "        self.y = Y.transpose((1,0,2))\n",
    "        self.len = len(X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1): \n",
    "        super(lstm_encoder, self).__init__()\n",
    "        \n",
    "        # LSTM Layer 초기화\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    # 모델의 순전파함수, 모델이 학습 및 예측 수행할 때 호출\n",
    "    def forward(self, x_input): # x_input : 모델의 입력 데이터\n",
    "        lstm_out, hidden = self.lstm(x_input)\n",
    "        return lstm_out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(lstm_decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        lstm_out, hidden = self.lstm(x_input, encoder_hidden_states)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_encoder_decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(lstm_encoder_decoder, self).__init__()\n",
    "        self.encoder = lstm_encoder(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.decoder = lstm_decoder(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, inputs, targets, target_len, teacher_forcing_ratio):\n",
    "        batch_size, _, input_size = inputs.shape\n",
    "        outputs = torch.zeros(batch_size, target_len, input_size)\n",
    "\n",
    "        _, hidden = self.encoder(inputs)\n",
    "        decoder_input = inputs[:,-1, :]\n",
    "\n",
    "        for t in range(target_len): \n",
    "            out, hidden = self.decoder(decoder_input.unsqueeze(1), hidden)\n",
    "            out = out.squeeze(1)\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                decoder_input = targets[:, t, :]\n",
    "            else:\n",
    "                decoder_input = out\n",
    "            outputs[:,t,:] = out\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, inputs, target_len):\n",
    "        self.eval()\n",
    "        batch_size, _, input_size = inputs.shape\n",
    "        outputs = torch.zeros(batch_size, target_len, input_size)\n",
    "        _, hidden = self.encoder(inputs)\n",
    "        decoder_input = inputs[:,-1, :]\n",
    "        for t in range(target_len): \n",
    "            out, hidden = self.decoder(decoder_input.unsqueeze(1), hidden)\n",
    "            out = out.squeeze(1)\n",
    "            decoder_input = out\n",
    "            outputs[:,t,:] = out\n",
    "        return outputs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "iw = 312\n",
    "ow = 104\n",
    "\n",
    "target_data = train_data['Close'].values  # 'Close' 열만을 대상으로 합니다.\n",
    "\n",
    "train_dataset = windowDataset(train_data, target_data, input_window=iw, output_window=ow, stride=1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_encoder_decoder(input_size=18, hidden_size=16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "epoch = 3000\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 18, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8768/1049911177.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asaf0\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8768/1113290679.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, targets, target_len, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asaf0\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8768/1404238968.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_input, encoder_hidden_states)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asaf0\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asaf0\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    770\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mc:\\Users\\asaf0\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    695\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m                            ):\n\u001b[1;32m--> 697\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0;32m    699\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
      "\u001b[1;32mc:\\Users\\asaf0\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    208\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m    211\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m    212\u001b[0m                     self.input_size, input.size(-1)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 18, got 1"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "with tqdm(range(epoch)) as tr:\n",
    "    for i in tr:\n",
    "        total_loss = 0.0\n",
    "        for x,y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            output = model(x, y, ow, 0.6).to(device)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.cpu().item()\n",
    "        tr.set_postfix(loss=\"{0:.5f}\".format(total_loss/len(train_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
